{"cells":[{"cell_type":"markdown","source":["# Clumping and Thresholding (C + T)\n","## Data\n","- Superpopulations: 'EUR' and 'AFR'\n","- Populations (similar to PS4)\n","  - EUR: CEU (training), GBR (validation), FIN (testing)\n","  - AFR: YRI (training), ESN (validation), GWD (testing)\n","- Genotypes: Chromosome 19 (1000 Genomes)\n","- Phenotypes (LDAK Simulation)\n","  - Power: -0.25, -1\n","  - Heritability: 0.1, 0.3, 0.5, 0.7, 0.9\n","  - Number of Causal SNPs: 1, 10, 100, 250, 512\n","  - Number of Phenotypes (Traits): 20\n","  - Effect Sizes: fixed (use LDL PRS data)\n","\n","## References\n","- Clumping and Thresholding: UCSD CSE 284 Lecture Slides, PS3, and PS4\n","- Phenotype Simulation: https://dougspeed.com/simulations\n","- LDL PRS Data: https://www.nature.com/articles/s41586-021-04064-3\n"],"metadata":{"id":"tzcSCKBTYLUM"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1073,"status":"ok","timestamp":1710181175943,"user":{"displayName":"Po-Chun Wu","userId":"13047107782036402947"},"user_tz":420},"id":"ipfNy5BN61rV","outputId":"cbe55474-e370-42d9-8804-35e2fd1a054f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1291,"status":"ok","timestamp":1710181678706,"user":{"displayName":"Po-Chun Wu","userId":"13047107782036402947"},"user_tz":420},"id":"h6q31sXow8et","outputId":"3c6c2027-745d-4df3-f4b0-009087357f2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-03-11 18:27:57--  https://s3.amazonaws.com/plink2-assets/alpha5/plink2_linux_x86_64_20240105.zip\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.34.30, 16.182.109.40, 52.216.54.16, ...\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.34.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 9266484 (8.8M) [application/zip]\n","Saving to: ‘plink2_linux_x86_64_20240105.zip’\n","\n","plink2_linux_x86_64 100%[===================>]   8.84M  31.7MB/s    in 0.3s    \n","\n","2024-03-11 18:27:58 (31.7 MB/s) - ‘plink2_linux_x86_64_20240105.zip’ saved [9266484/9266484]\n","\n","Archive:  plink2_linux_x86_64_20240105.zip\n","  inflating: plink2                  \n","PLINK v2.00a5.10LM 64-bit Intel (5 Jan 2024)   www.cog-genomics.org/plink/2.0/\n","(C) 2005-2024 Shaun Purcell, Christopher Chang   GNU General Public License v3\n","\n","  plink2 <input flag(s)...> [command flag(s)...] [other flag(s)...]\n","  plink2 --help [flag name(s)...]\n","\n","Commands include --rm-dup list, --make-bpgen, --export, --freq, --geno-counts,\n","--sample-counts, --missing, --hardy, --het, --fst, --indep-pairwise, --ld,\n","--sample-diff, --make-king, --king-cutoff, --pmerge, --pgen-diff,\n","--write-samples, --write-snplist, --make-grm-list, --pca, --glm, --adjust-file,\n","--gwas-ssf, --clump, --score, --variant-score, --genotyping-rate, --pgen-info,\n","--validate, and --zst-decompress.\n","\n","\"plink2 --help | more\" describes all functions.\n"]}],"source":["! wget https://s3.amazonaws.com/plink2-assets/alpha5/plink2_linux_x86_64_20240105.zip\n","! unzip plink2_linux_x86_64_20240105.zip\n","! ./plink2"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710181680147,"user":{"displayName":"Po-Chun Wu","userId":"13047107782036402947"},"user_tz":420},"id":"4QPf_plu8c7O","outputId":"8d8f46aa-610e-4b4c-b145-c580d9020c59"},"outputs":[{"output_type":"stream","name":"stdout","text":["Populating the interactive namespace from numpy and matplotlib\n"]}],"source":["# Import libraries\n","%pylab inline\n","import os\n","import csv\n","import scipy.stats\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["! cd /content/drive/MyDrive/CSE-284-Final-Project/data/CT/superpopulation/ | ls -1 | wc -l"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d8vQ-wywymuw","executionInfo":{"status":"ok","timestamp":1710182039300,"user_tz":420,"elapsed":265,"user":{"displayName":"Po-Chun Wu","userId":"13047107782036402947"}},"outputId":"2721cf5e-5b7b-4671-9c7b-0db72703c1ee"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["635\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"id":"e_7qXiCENvGm","executionInfo":{"status":"ok","timestamp":1710181681624,"user_tz":420,"elapsed":186,"user":{"displayName":"Po-Chun Wu","userId":"13047107782036402947"}}},"outputs":[],"source":["# POP_LIST = ['EUR', 'AFR']\n","# POWER_LIST = [-0.25, -1]\n","# HER_LIST = [0.1, 0.3, 0.5, 0.7, 0.9]\n","# NUM_CAUSALS_LIST = [1, 10, 100, 250, 512]\n","# PHENO_NUM_START = 1\n","# PHENO_NUM_END = 20\n","\n","\n","POP_LIST = ['EUR']\n","POP_TRAIN = 'CEU'\n","POP_VAL = 'GBR'\n","POP_TEST = 'FIN'\n","POWER_LIST = [-0.25, -1]\n","HER_LIST = [0.1, 0.3, 0.5, 0.7, 0.9]\n","NUM_CAUSALS_LIST = [1, 10, 100, 250, 512]\n","PHENO_NUM_START = 1\n","PHENO_NUM_END = 20"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qph4kZ5jjIbB","outputId":"0283ac30-9e7d-4ece-e076-48f604edd883","executionInfo":{"status":"ok","timestamp":1710181827271,"user_tz":420,"elapsed":62782,"user":{"displayName":"Po-Chun Wu","userId":"13047107782036402947"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["PLINK v2.00a5.10LM 64-bit Intel (5 Jan 2024)   www.cog-genomics.org/plink/2.0/\n","(C) 2005-2024 Shaun Purcell, Christopher Chang   GNU General Public License v3\n","Logging to EUR_train.log.\n","Options in effect:\n","  --bfile /content/chr19\n","  --keep EUR_train_IID_only.txt\n","  --make-bed\n","  --out EUR_train\n","\n","Start time: Mon Mar 11 18:29:26 2024\n","12978 MiB RAM detected, ~11767 available; reserving 6489 MiB for main\n","workspace.\n","Using up to 2 compute threads.\n","2504 samples (1270 females, 1234 males; 2497 founders) loaded from\n","/content/chr19.fam.\n","283173 variants loaded from /content/chr19.bim.\n","Note: No phenotype data present.\n","--keep: 99 samples remaining.\n","99 samples (50 females, 49 males; 99 founders) remaining after main filters.\n","Writing EUR_train.fam ... done.\n","Writing EUR_train.bim ... done.\n","Writing EUR_train.bed ... 0%\b\b23%\b\b\b46%\b\b\b69%\b\b\b92%\b\b\bdone.\n","End time: Mon Mar 11 18:29:27 2024\n","PLINK v2.00a5.10LM 64-bit Intel (5 Jan 2024)   www.cog-genomics.org/plink/2.0/\n","(C) 2005-2024 Shaun Purcell, Christopher Chang   GNU General Public License v3\n","Logging to EUR_val.log.\n","Options in effect:\n","  --bfile /content/chr19\n","  --keep EUR_val_IID_only.txt\n","  --make-bed\n","  --out EUR_val\n","\n","Start time: Mon Mar 11 18:29:27 2024\n","12978 MiB RAM detected, ~11759 available; reserving 6489 MiB for main\n","workspace.\n","Using up to 2 compute threads.\n","2504 samples (1270 females, 1234 males; 2497 founders) loaded from\n","/content/chr19.fam.\n","283173 variants loaded from /content/chr19.bim.\n","Note: No phenotype data present.\n","--keep: 91 samples remaining.\n","91 samples (45 females, 46 males; 91 founders) remaining after main filters.\n","Writing EUR_val.fam ... done.\n","Writing EUR_val.bim ... done.\n","Writing EUR_val.bed ... 0%\b\b23%\b\b\b46%\b\b\b69%\b\b\b92%\b\b\bdone.\n","End time: Mon Mar 11 18:29:27 2024\n","PLINK v2.00a5.10LM 64-bit Intel (5 Jan 2024)   www.cog-genomics.org/plink/2.0/\n","(C) 2005-2024 Shaun Purcell, Christopher Chang   GNU General Public License v3\n","Logging to EUR_test.log.\n","Options in effect:\n","  --bfile /content/chr19\n","  --keep EUR_test_IID_only.txt\n","  --make-bed\n","  --out EUR_test\n","\n","Start time: Mon Mar 11 18:29:27 2024\n","12978 MiB RAM detected, ~11758 available; reserving 6489 MiB for main\n","workspace.\n","Using up to 2 compute threads.\n","2504 samples (1270 females, 1234 males; 2497 founders) loaded from\n","/content/chr19.fam.\n","283173 variants loaded from /content/chr19.bim.\n","Note: No phenotype data present.\n","--keep: 99 samples remaining.\n","99 samples (61 females, 38 males; 99 founders) remaining after main filters.\n","Writing EUR_test.fam ... done.\n","Writing EUR_test.bim ... done.\n","Writing EUR_test.bed ... 0%\b\b23%\b\b\b46%\b\b\b69%\b\b\b92%\b\b\bdone.\n","End time: Mon Mar 11 18:29:27 2024\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137506 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137506 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137506 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9806492434819399\n","Validation: \n","Best:  0.000001 0.47637654688406017\n","Testing: \n","pval=0.000001, R2=0.5064926222541352\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137517 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137517 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137517 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9807173268469727\n","Validation: \n","Best:  0.00001 0.49039615420003596\n","Testing: \n","pval=0.00001, R2=0.5616905412538508\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137522 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137522 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137522 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9788810404348964\n","Validation: \n","Best:  0.000001 0.47963081594239276\n","Testing: \n","pval=0.000001, R2=0.5211640186868229\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137516 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137516 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137516 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9772573206892313\n","Validation: \n","Best:  0.000001 0.4710467307755349\n","Testing: \n","pval=0.000001, R2=0.5431908247036804\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137460 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137460 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137460 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9792189962565166\n","Validation: \n","Best:  0.000001 0.40639127066581004\n","Testing: \n","pval=0.000001, R2=0.5826878251292071\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137510 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137510 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137510 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9806920097465481\n","Validation: \n","Best:  0.000001 0.44748136135704475\n","Testing: \n","pval=0.000001, R2=0.42177084860568714\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137493 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137493 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137493 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9758247110592972\n","Validation: \n","Best:  0.000001 0.45872582302955744\n","Testing: \n","pval=0.000001, R2=0.5688818622094488\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137503 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137503 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137503 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9762313873847172\n","Validation: \n","Best:  0.000001 0.45637701575199857\n","Testing: \n","pval=0.000001, R2=0.5332039238853521\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137502 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137502 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137502 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9750484968737646\n","Validation: \n","Best:  0.00001 0.22773573567570324\n","Testing: \n","pval=0.00001, R2=0.17296206488712645\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137513 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137513 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137513 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9790898808439896\n","Validation: \n","Best:  0.000001 0.42910955207559953\n","Testing: \n","pval=0.000001, R2=0.5490967623976267\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137538 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137538 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137538 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9764775713988434\n","Validation: \n","Best:  0.000001 0.30070707225843113\n","Testing: \n","pval=0.000001, R2=0.34999330930537376\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137545 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137545 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137545 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9778054412547543\n","Validation: \n","Best:  0.000001 0.5271976706297862\n","Testing: \n","pval=0.000001, R2=0.5739446720895706\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137515 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137515 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137515 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9785282320573658\n","Validation: \n","Best:  0.000001 0.5476973354778164\n","Testing: \n","pval=0.000001, R2=0.52308258280751\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137522 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137522 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137522 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9798346751341535\n","Validation: \n","Best:  0.000001 0.4884175348850941\n","Testing: \n","pval=0.000001, R2=0.4988205695176828\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137521 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137521 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137521 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9831545557495502\n","Validation: \n","Best:  0.000001 0.5177658655943581\n","Testing: \n","pval=0.000001, R2=0.5120644995974833\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137493 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137493 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137493 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9790483027199206\n","Validation: \n","Best:  0.000001 0.47737885039732614\n","Testing: \n","pval=0.000001, R2=0.5075338451844075\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137482 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137482 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137482 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9804063073055789\n","Validation: \n","Best:  0.000001 0.4946515070804783\n","Testing: \n","pval=0.000001, R2=0.5237942801683573\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137499 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137499 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137499 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9780732145061574\n","Validation: \n","Best:  0.000001 0.26925287870928755\n","Testing: \n","pval=0.000001, R2=0.3080475795594043\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137522 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137522 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137522 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9790933966215192\n","Validation: \n","Best:  0.00001 0.345560330718527\n","Testing: \n","pval=0.00001, R2=0.37761497549106404\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137535 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137535 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Warning: 1 line skipped in --q-score-range data file.\n","Warning: 137535 --score file entries were skipped due to missing variant IDs.\n","(Add the 'list-variants' modifier to see which variants were actually used for\n","scoring.)\n","Training: \n","0.9758567006876632\n","Validation: \n","Best:  0.000001 0.4488118158709658\n","Testing: \n","pval=0.000001, R2=0.574168227318614\n","Combined prediction data has been written to: combined_predict_EUR_power=-0.25_her=0.9_num-causals=512_pheno=1-20.csv\n","Combined result data has been written to: combined_result_EUR_power=-0.25_her=0.9_num-causals=512_pheno=1-20.csv\n","          IID  predict    actual  pheno_num\n","0     HG00171      0.0 -0.313430          1\n","1     HG00173      0.0  0.481810          1\n","2     HG00174      0.0 -0.766593          1\n","3     HG00176      0.0 -1.266699          1\n","4     HG00177      0.0 -0.645353          1\n","...       ...      ...       ...        ...\n","1975  HG00380      0.0 -0.731406         20\n","1976  HG00381      0.0 -0.870224         20\n","1977  HG00382      0.0 -0.892427         20\n","1978  HG00383      0.0  0.933604         20\n","1979  HG00384      0.0 -0.869691         20\n","\n","[1980 rows x 4 columns]\n","   population/superpopulation  power  her  num_causals  pheno_num     train  \\\n","0                         EUR  -0.25  0.9          512          1  0.980649   \n","1                         EUR  -0.25  0.9          512          2  0.980717   \n","2                         EUR  -0.25  0.9          512          3  0.978881   \n","3                         EUR  -0.25  0.9          512          4  0.977257   \n","4                         EUR  -0.25  0.9          512          5  0.979219   \n","5                         EUR  -0.25  0.9          512          6  0.980692   \n","6                         EUR  -0.25  0.9          512          7  0.975825   \n","7                         EUR  -0.25  0.9          512          8  0.976231   \n","8                         EUR  -0.25  0.9          512          9  0.975048   \n","9                         EUR  -0.25  0.9          512         10  0.979090   \n","10                        EUR  -0.25  0.9          512         11  0.976478   \n","11                        EUR  -0.25  0.9          512         12  0.977805   \n","12                        EUR  -0.25  0.9          512         13  0.978528   \n","13                        EUR  -0.25  0.9          512         14  0.979835   \n","14                        EUR  -0.25  0.9          512         15  0.983155   \n","15                        EUR  -0.25  0.9          512         16  0.979048   \n","16                        EUR  -0.25  0.9          512         17  0.980406   \n","17                        EUR  -0.25  0.9          512         18  0.978073   \n","18                        EUR  -0.25  0.9          512         19  0.979093   \n","19                        EUR  -0.25  0.9          512         20  0.975857   \n","\n","         val      test  \n","0   0.476377  0.506493  \n","1   0.490396  0.561691  \n","2   0.479631  0.521164  \n","3   0.471047  0.543191  \n","4   0.406391  0.582688  \n","5   0.447481  0.421771  \n","6   0.458726  0.568882  \n","7   0.456377  0.533204  \n","8   0.227736  0.172962  \n","9   0.429110  0.549097  \n","10  0.300707  0.349993  \n","11  0.527198  0.573945  \n","12  0.547697  0.523083  \n","13  0.488418  0.498821  \n","14  0.517766  0.512064  \n","15  0.477379  0.507534  \n","16  0.494652  0.523794  \n","17  0.269253  0.308048  \n","18  0.345560  0.377615  \n","19  0.448812  0.574168  \n"]}],"source":["! cp /content/drive/MyDrive/CSE-284-Final-Project/data/1000g_by_chrom/chr19.* .\n","# ! cp /content/drive/MyDrive/CSE-284-Final-Project/data/1000g_by_population/{POP_TRAIN}_all* .\n","# ! cp /content/drive/MyDrive/CSE-284-Final-Project/data/1000g_by_population/{POP_VAL}_all* .\n","# ! cp /content/drive/MyDrive/CSE-284-Final-Project/data/1000g_by_population/{POP_TEST}_all* .\n","\n","PREFIX = '/content'\n","chr19 = f'{PREFIX}/chr19'\n","\n","! echo \"0.000001 0 0.000001\" > range_list\n","! echo \"0.00001 0 0.00001\" >> range_list\n","! echo \"0.0001 0 0.0001\" >> range_list\n","! echo \"0.001 0 0.001\" >> range_list\n","! echo \"0.05 0 0.05\" >> range_list\n","! echo \"0.1 0 0.1\" >> range_list\n","\n","for POP in POP_LIST:\n","\n","  ! cp /content/drive/MyDrive/CSE-284-Final-Project/data/split_ids/by_population/{POP_TRAIN}_all.txt .\n","  ! cp /content/drive/MyDrive/CSE-284-Final-Project/data/split_ids/by_population/{POP_VAL}_all.txt .\n","  ! cp /content/drive/MyDrive/CSE-284-Final-Project/data/split_ids/by_population/{POP_TEST}_all.txt .\n","  ! cut -d ' ' -f 2 {POP_TRAIN}_all.txt > {POP}_train_IID_only.txt\n","  ! cut -d ' ' -f 2 {POP_VAL}_all.txt > {POP}_val_IID_only.txt\n","  ! cut -d ' ' -f 2 {POP_TEST}_all.txt > {POP}_test_IID_only.txt\n","\n","  keep_train = f'{POP}_train_IID_only.txt'\n","  out_train = f'{POP}_train'\n","\n","  keep_val = f'{POP}_val_IID_only.txt'\n","  out_val = f'{POP}_val'\n","\n","  keep_test = f'{POP}_test_IID_only.txt'\n","  out_test = f'{POP}_test'\n","\n","  # Make bed\n","\n","  sh = f'''\n","  ./plink2 \\\n","    --make-bed \\\n","    --bfile {chr19} \\\n","    --keep {keep_train} \\\n","    --out {out_train}\n","\n","  ./plink2 \\\n","    --make-bed \\\n","    --bfile {chr19} \\\n","    --keep {keep_val} \\\n","    --out {out_val}\n","\n","  ./plink2 \\\n","    --make-bed \\\n","    --bfile {chr19} \\\n","    --keep {keep_test} \\\n","    --out {out_test}\n","\n","  '''\n","  with open('makebed.sh', 'w') as file:\n","      file.write(sh)\n","  !bash makebed.sh\n","\n","  GENO_TRAIN = f'{PREFIX}/{POP}_train'\n","  GENO_VAL = f'{PREFIX}/{POP}_val'\n","  GENO_TEST = f'{PREFIX}/{POP}_test'\n","\n","  for POWER in POWER_LIST:\n","    for HER in HER_LIST:\n","      for NUM_CAUSALS in NUM_CAUSALS_LIST:\n","        ! cp /content/drive/MyDrive/CSE-284-Final-Project/data/chr19_ldl_pheno/power={POWER}_her={HER}_num-causals={NUM_CAUSALS}.pheno .\n","        PHENO = f'{PREFIX}/power={POWER}_her={HER}_num-causals={NUM_CAUSALS}.pheno'\n","\n","        # GWAS (GLM)\n","        sh = f'''\n","        ./plink2 --bfile {GENO_TRAIN} \\\n","                --pheno {PHENO} \\\n","                --glm allow-no-covars \\\n","                --maf 0.05 \\\n","                --out {POP}_gwas \\\n","                --silent\n","        '''\n","\n","        with open('gwas.sh', 'w') as file:\n","            file.write(sh)\n","        !bash gwas.sh\n","\n","\n","        combined_prediction_data = []\n","        combined_result_data = []\n","\n","        for PHENO_NUM in range(PHENO_NUM_START, PHENO_NUM_END + 1):\n","\n","            GLM = f'/content/{POP}_gwas.PHENO{PHENO_NUM}.glm.linear'\n","\n","            # Clumping\n","\n","            sh = f'''\n","            ./plink2 \\\n","                --bfile {GENO_TRAIN} \\\n","                --clump-p1 1 \\\n","                --clump-r2 0.1 \\\n","                --clump-kb 250 \\\n","                --clump {GLM} \\\n","                --out {POP}_gwas.PHENO{PHENO_NUM} \\\n","                --silent\n","\n","            awk 'NR!=1{{print $3}}' {POP}_gwas.PHENO{PHENO_NUM}.clumps > {POP}_gwas.PHENO{PHENO_NUM}.valid.snp\n","            cat {POP}_gwas.PHENO{PHENO_NUM}.clumps | awk '{{print $3 \"\\t\" $4}}' > {POP}_gwas.PHENO{PHENO_NUM}.pvals\n","            '''\n","\n","            with open('clump.sh', 'w') as file:\n","                file.write(sh)\n","            !bash clump.sh\n","\n","            GLM = f'/content/{POP}_gwas.PHENO{PHENO_NUM}.glm.linear'\n","            PVALS = f'/content/{POP}_gwas.PHENO{PHENO_NUM}.pvals'\n","            VAL = f'/content/{POP}_gwas.PHENO{PHENO_NUM}.valid.snp'\n","\n","            # Scoring\n","\n","            sh = f'''\n","\n","            ./plink2 \\\n","            --bfile {GENO_TRAIN} \\\n","            --score {GLM} 3 7 12 header cols=+scoresums\\\n","            --q-score-range range_list {PVALS} \\\n","            --extract {VAL} \\\n","            --out {POP}_PHENO{PHENO_NUM}_train \\\n","            --silent\n","\n","            ./plink2 \\\n","            --bfile {GENO_VAL} \\\n","            --score {GLM} 3 7 12 header cols=+scoresums\\\n","            --q-score-range range_list {PVALS} \\\n","            --extract {VAL} \\\n","            --out {POP}_PHENO{PHENO_NUM}_val \\\n","            --silent\n","\n","            ./plink2 \\\n","            --bfile {GENO_TEST} \\\n","            --score {GLM} 3 7 12 header cols=+scoresums\\\n","            --q-score-range range_list {PVALS} \\\n","            --extract {VAL} \\\n","            --out {POP}_PHENO{PHENO_NUM}_test \\\n","            --silent\n","            '''\n","\n","            with open('score.sh', 'w') as file:\n","              file.write(sh)\n","            !bash score.sh\n","\n","            # Thresholding\n","\n","            pval_list = [\"0.000001\", \"0.00001\", \"0.0001\", \"0.001\", \"0.05\", \"0.1\"]\n","            best_pval_idx = 0\n","            train_scores = []\n","            val_scores = []\n","            test_scores = []\n","            train_score = 0\n","            val_score = 0\n","            test_score = 0\n","\n","            print(\"Training: \")\n","            for pval in pval_list:\n","                score_file = f'{POP}_PHENO{PHENO_NUM}_train.{pval}.sscore'\n","                if os.path.isfile(score_file) == False:\n","                  continue\n","                prs = pd.read_csv(score_file, delim_whitespace=True)\n","                phen = pd.read_csv(f'{PHENO}',\n","                              delim_whitespace=True, usecols=[0, 1, PHENO_NUM + 1], names=[\"FID\",\"IID\",\"phen\"])\n","                d = pd.merge(prs, phen, on=[\"IID\"])\n","                score = scipy.stats.pearsonr(d[\"phen\"], d[\"SCORE1_SUM\"])[0] ** 2\n","                train_scores.append(score)\n","                # print(\"pval=%s, R2=%s\"%(pval, score))\n","            train_score = np.nanmax(train_scores)\n","            print(train_score)\n","\n","            print(\"Validation: \")\n","            for pval in pval_list:\n","                score_file = f'{POP}_PHENO{PHENO_NUM}_val.{pval}.sscore'\n","                if os.path.isfile(score_file) == False:\n","                  val_scores.append(-1)\n","                  continue\n","                prs = pd.read_csv(score_file, delim_whitespace=True)\n","                phen = pd.read_csv(f'{PHENO}',\n","                              delim_whitespace=True, usecols=[0, 1, PHENO_NUM + 1], names=[\"FID\",\"IID\",\"phen\"])\n","                d = pd.merge(prs, phen, on=[\"IID\"])\n","                score = scipy.stats.pearsonr(d[\"phen\"], d[\"SCORE1_SUM\"])[0] ** 2\n","                val_scores.append(score)\n","                # print(\"pval=%s, R2=%s\"%(pval, score))\n","            val_score = np.nanmax(val_scores)\n","            best_pval_idx = val_scores.index(val_score)\n","            print(\"Best: \", pval_list[best_pval_idx], val_score)\n","\n","            print(\"Testing: \")\n","            best_pval = pval_list[best_pval_idx]\n","            prs = pd.read_csv(f'{POP}_PHENO{PHENO_NUM}_test.{best_pval}.sscore', delim_whitespace=True)\n","            phen = pd.read_csv(f'{PHENO}',\n","                              delim_whitespace=True, usecols=[0, 1, PHENO_NUM + 1], names=[\"FID\",\"IID\",\"phen\"])\n","            d = pd.merge(prs, phen, on=[\"IID\"])\n","            test_score = scipy.stats.pearsonr(d[\"phen\"], d[\"SCORE1_SUM\"])[0] ** 2\n","            print(\"pval=%s, R2=%s\"%(best_pval, test_score))\n","            predict_df = d[['IID', 'SCORE1_SUM', 'phen']].copy()\n","            predict_df.rename(columns={'IID': 'IID', 'SCORE1_SUM': 'predict', 'phen': 'actual'}, inplace=True)\n","            predict_df[\"pheno_num\"] = PHENO_NUM\n","            combined_prediction_data.append(predict_df)\n","\n","            result_dict = {\n","                \"population/superpopulation\": POP,\n","                \"power\": POWER,\n","                \"her\": HER,\n","                \"num_causals\": NUM_CAUSALS,\n","                \"pheno_num\": PHENO_NUM,\n","                \"train\": train_score,\n","                \"val\": val_score,\n","                \"test\": test_score\n","            }\n","            combined_result_data.append(result_dict)\n","\n","        combined_prediction_df = pd.concat(combined_prediction_data, ignore_index=True)\n","        combined_result_df = pd.DataFrame(combined_result_data)\n","\n","        # Write combined prediction data to file\n","        combined_prediction_file = f'combined_predict_{POP}_power={POWER}_her={HER}_num-causals={NUM_CAUSALS}_pheno={PHENO_NUM_START}-{PHENO_NUM_END}.csv'\n","        combined_prediction_df.to_csv(combined_prediction_file, index=False)\n","        print(\"Combined prediction data has been written to:\", combined_prediction_file)\n","\n","        # Write combined result data to file\n","        combined_result_file = f'combined_result_{POP}_power={POWER}_her={HER}_num-causals={NUM_CAUSALS}_pheno={PHENO_NUM_START}-{PHENO_NUM_END}.csv'\n","        combined_result_df.to_csv(combined_result_file, index=False)\n","        print(\"Combined result data has been written to:\", combined_result_file)\n","\n","        print(combined_prediction_df)\n","        print(combined_result_df)\n","\n","        ! cp combined_predict_{POP}_power={POWER}_her={HER}_num-causals={NUM_CAUSALS}_pheno={PHENO_NUM_START}-{PHENO_NUM_END}.csv /content/drive/MyDrive/CSE-284-Final-Project/data/CT/population\n","        ! cp combined_result_{POP}_power={POWER}_her={HER}_num-causals={NUM_CAUSALS}_pheno={PHENO_NUM_START}-{PHENO_NUM_END}.csv /content/drive/MyDrive/CSE-284-Final-Project/data/CT/population\n","\n"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[],"authorship_tag":"ABX9TyNt5ByncLqkKd4qcsW3JRyU"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}